{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZuXNIJTmF0/Kcc/kZTi0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britbrat0/cs676/blob/main/project1_deliverable1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7ErxJXeAhc2",
        "outputId": "f558d7c4-c5bc-4eb4-bc51-d9e9ce6f31f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tldextract\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from tldextract) (2.32.4)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract) (3.19.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.1.0->tldextract) (2025.8.3)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.3.0\n"
          ]
        }
      ],
      "source": [
        "%pip install tldextract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# FEATURE EXTRACTION\n",
        "# --------------------------------------------------------\n",
        "def extract_features(url: str = None, text_input: str = None):\n",
        "    \"\"\"\n",
        "    Extract credibility-related features.\n",
        "    Returns scores + details including warnings and errors.\n",
        "    \"\"\"\n",
        "    features = {\n",
        "        \"scores\": {\n",
        "            \"source_authority\": 0,\n",
        "            \"publication_quality\": 0,\n",
        "            \"citation_patterns\": 0,\n",
        "            \"content_accuracy\": 0\n",
        "        },\n",
        "        \"details\": {}\n",
        "    }\n",
        "\n",
        "    text = \"\"\n",
        "    soup = None\n",
        "\n",
        "    # Fetch content if URL is provided\n",
        "    if url:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "            response.raise_for_status()\n",
        "            html = response.text\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            text = soup.get_text(separator=\" \", strip=True)\n",
        "        except Exception as e:\n",
        "            features[\"details\"][\"error\"] = f\"Failed to fetch URL: {str(e)}\"\n",
        "            return features\n",
        "    else:\n",
        "        text = text_input or \"\"\n",
        "        if not text.strip():\n",
        "            features[\"details\"][\"error\"] = \"No content provided.\"\n",
        "            return features\n",
        "\n",
        "    # --- 1. Source Authority ---\n",
        "    if url:\n",
        "        domain_info = tldextract.extract(url)\n",
        "        domain = f\"{domain_info.domain}.{domain_info.suffix}\"\n",
        "        features[\"details\"][\"domain\"] = domain\n",
        "\n",
        "        if domain_info.suffix in [\"gov\", \"edu\"]:\n",
        "            authority_score = 9\n",
        "        elif domain_info.suffix in [\"org\"]:\n",
        "            authority_score = 7\n",
        "        else:\n",
        "            authority_score = 5\n",
        "    else:\n",
        "        authority_score = 5\n",
        "\n",
        "    features[\"scores\"][\"source_authority\"] = authority_score\n",
        "\n",
        "    # --- 2. Publication Quality ---\n",
        "    word_count = len(text.split())\n",
        "    ads = 0 if soup is None else (len(soup.find_all(\"iframe\")) + len(soup.find_all(\"script\")))\n",
        "\n",
        "    quality_score = 5\n",
        "    if word_count > 800:\n",
        "        quality_score += 2\n",
        "    if ads > 10:\n",
        "        quality_score -= 2\n",
        "    quality_score = max(0, min(10, quality_score))\n",
        "\n",
        "    features[\"scores\"][\"publication_quality\"] = quality_score\n",
        "    features[\"details\"][\"word_count\"] = word_count\n",
        "\n",
        "    # --- 3. Citation Patterns ---\n",
        "    if soup:\n",
        "        links = [a['href'] for a in soup.find_all(\"a\", href=True)]\n",
        "        domain = features[\"details\"].get(\"domain\", \"\")\n",
        "        external_links = [l for l in links if domain not in l]\n",
        "        citation_score = min(10, len(external_links))\n",
        "        features[\"details\"][\"external_links_count\"] = len(external_links)\n",
        "    else:\n",
        "        citation_score = 3\n",
        "\n",
        "    features[\"scores\"][\"citation_patterns\"] = citation_score\n",
        "\n",
        "    # --- 4. Content Accuracy (heuristics) ---\n",
        "    suspicious_keywords = [\"miracle cure\", \"shocking secret\", \"click here\", \"you won’t believe\"]\n",
        "    suspicious_hits = sum([text.lower().count(kw) for kw in suspicious_keywords])\n",
        "\n",
        "    accuracy_score = 8\n",
        "    if suspicious_hits > 0:\n",
        "        accuracy_score -= suspicious_hits\n",
        "    accuracy_score = max(0, min(10, accuracy_score))\n",
        "\n",
        "    features[\"scores\"][\"content_accuracy\"] = accuracy_score\n",
        "    features[\"details\"][\"suspicious_keywords_found\"] = suspicious_hits\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# RULE-BASED SCORING\n",
        "# --------------------------------------------------------\n",
        "def compute_rule_based_score(features):\n",
        "    weights = {\n",
        "        \"source_authority\": 0.3,\n",
        "        \"publication_quality\": 0.25,\n",
        "        \"citation_patterns\": 0.25,\n",
        "        \"content_accuracy\": 0.2\n",
        "    }\n",
        "    score = sum(features[\"scores\"][f] * w for f, w in weights.items())\n",
        "    return round(score / 10, 2)  # normalize to 0–1\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# HYBRID CREDIBILITY ASSESSOR\n",
        "# --------------------------------------------------------\n",
        "def assess_credibility(url: str = None, text: str = None, model_path=\"credibility_model.pkl\"):\n",
        "    \"\"\"\n",
        "    Hybrid credibility assessment returning simplified JSON:\n",
        "    { \"score\": float, \"explanation\": string }\n",
        "    \"\"\"\n",
        "    features = extract_features(url=url, text_input=text)\n",
        "\n",
        "    # If error fetching data\n",
        "    if \"error\" in features[\"details\"]:\n",
        "        return json.dumps({\n",
        "            \"score\": 0.0,\n",
        "            \"explanation\": f\"Analysis failed: {features['details']['error']}\"\n",
        "        }, indent=2)\n",
        "\n",
        "    rule_score = compute_rule_based_score(features)\n",
        "    ml_score = None\n",
        "\n",
        "    # Try ML model\n",
        "    if os.path.exists(model_path):\n",
        "        try:\n",
        "            model = joblib.load(model_path)\n",
        "            X = np.array([[\n",
        "                features[\"scores\"][\"source_authority\"],\n",
        "                features[\"scores\"][\"publication_quality\"],\n",
        "                features[\"scores\"][\"citation_patterns\"],\n",
        "                features[\"scores\"][\"content_accuracy\"]\n",
        "            ]])\n",
        "            ml_score = float(model.predict(X)[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Hybrid combination\n",
        "    if ml_score is not None:\n",
        "        final_score = 0.4 * rule_score + 0.6 * ml_score\n",
        "    else:\n",
        "        final_score = rule_score\n",
        "\n",
        "    # --- Build Explanation ---\n",
        "    explanation_parts = []\n",
        "    if features[\"scores\"][\"source_authority\"] >= 7:\n",
        "        explanation_parts.append(\"source has strong domain authority\")\n",
        "    if features[\"scores\"][\"citation_patterns\"] > 5:\n",
        "        explanation_parts.append(\"it provides external references\")\n",
        "    if features[\"scores\"][\"content_accuracy\"] < 5:\n",
        "        explanation_parts.append(\"content shows potential bias or suspicious claims\")\n",
        "    if features[\"scores\"][\"publication_quality\"] > 6:\n",
        "        explanation_parts.append(\"article is detailed and well-structured\")\n",
        "\n",
        "    if not explanation_parts:\n",
        "        explanation = \"Credibility is assessed based on available features.\"\n",
        "    else:\n",
        "        explanation = \"This source is considered credible because \" + \", and \".join(explanation_parts) + \".\"\n",
        "\n",
        "    return json.dumps({\n",
        "        \"score\": round(final_score, 2),\n",
        "        \"explanation\": explanation\n",
        "    }, indent=2)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# DEMO\n",
        "# --------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example URL\n",
        "    print(assess_credibility(url=\"https://www.bbc.com/news/science-environment-123456\"))\n",
        "\n",
        "    # Example raw text\n",
        "    sample_text = \"Scientists discovered a shocking secret cure that the government doesn’t want you to know!\"\n",
        "    print(assess_credibility(text=sample_text))\n",
        "\n",
        "    # Example URL\n",
        "    print(assess_credibility(url=\"https://www.bbc.com/news/world-us-canada-55568621\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuPS0KfmAhz8",
        "outputId": "1df9154e-0d3f-4bfb-a5e3-5b2b8fabd5e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"score\": 0.0,\n",
            "  \"explanation\": \"Analysis failed: Failed to fetch URL: 404 Client Error: Not Found for url: https://www.bbc.com/news/science-environment-123456\"\n",
            "}\n",
            "{\n",
            "  \"score\": 0.49,\n",
            "  \"explanation\": \"Credibility is assessed based on available features.\"\n",
            "}\n",
            "{\n",
            "  \"score\": 0.68,\n",
            "  \"explanation\": \"This source is considered credible because it provides external references.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkknRsAxAxgP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}