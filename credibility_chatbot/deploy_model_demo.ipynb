{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credibility Scoring Chatbot Project\n",
        "\n",
        "This notebook demonstrates the development and deployment of a hybrid credibility scoring model for URLs and web content.  \n",
        "It includes:\n",
        "\n",
        "1. **Hybrid credibility model**: Combines rule-based signals (content length, domain authority, citation patterns) with ML predictions.  \n",
        "2. **Streamlit chatbot integration**: Answers questions using GPT models, incorporates web search results, and provides credibility scores.  \n",
        "3. **Deployment demonstration**: Shows how to deploy the trained ML model artifact (`credibility_model.pkl`) to Hugging Face Hub and use it reproducibly.  \n",
        "\n",
        "The workflow is fully reproducible and demonstrates production-ready capabilities.\n"
      ],
      "metadata": {
        "id": "UCX-h_8lld0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrating Model Deployment\n",
        "\n",
        "This section shows how the trained credibility scoring model (`credibility_model.pkl`) can be deployed, shared, and used reproducibly:\n",
        "\n",
        "1. **Deployment to Hugging Face Hub:**  \n",
        "   - The trained model artifact is uploaded to a Hugging Face repository.  \n",
        "   - Allows anyone to download and use the model without needing the original notebook or local files.\n",
        "\n",
        "2. **Downloading the Model:**  \n",
        "   - Using the `huggingface_hub` library, the model can be programmatically downloaded from the Hub.  \n",
        "   - Ensures portability across environments.\n",
        "\n",
        "3. **Loading and Using the Model:**  \n",
        "   - The `.pkl` file is loaded using Pythonâ€™s `pickle` module.  \n",
        "   - A sample feature vector is used to demonstrate a prediction, verifying functionality.\n",
        "\n",
        "4. **Environment Security:**  \n",
        "   - The Hugging Face token is loaded from an environment variable (`HF_TOKEN`) to avoid exposing secrets.  \n",
        "   - This ensures safe and reproducible usage.\n",
        "\n",
        "This workflow demonstrates that the model is **deployment-ready**, reproducible, and can be integrated into downstream applications like the Streamlit chatbot.\n"
      ],
      "metadata": {
        "id": "yribDZ_Tlgqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Hugging Face Model Deployment Demo\n",
        "# -----------------------------\n",
        "# This cell downloads the trained model from Hugging Face Hub,\n",
        "# loads it in Python, and performs a test prediction.\n",
        "# -----------------------------\n",
        "\n",
        "# Install Hugging Face Hub if needed\n",
        "!pip install huggingface_hub --quiet\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Hugging Face repository info\n",
        "# -----------------------------\n",
        "repo_id = \"your-username/credibility-model\"  # Replace with your HF repo\n",
        "hf_token = os.getenv(\"HF_TOKEN\")  # safer than hardcoding your token\n",
        "\n",
        "if not hf_token:\n",
        "    print(\"Set your HF_TOKEN environment variable before running this cell.\")\n",
        "else:\n",
        "    # -----------------------------\n",
        "    # Download the model from HF Hub\n",
        "    # -----------------------------\n",
        "    model_path = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=\"credibility_model.pkl\",\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load the model\n",
        "    # -----------------------------\n",
        "    with open(model_path, \"rb\") as f:\n",
        "        credibility_model = pickle.load(f)\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Test a sample prediction\n",
        "    # -----------------------------\n",
        "    sample_features = [[1000, 15]]  # Example: content length, domain length\n",
        "    pred_score = credibility_model.predict(sample_features)[0]\n",
        "    print(\"Predicted credibility score:\", pred_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njKzc-nolgON",
        "outputId": "9d90462a-4be3-4a39-bfd1-58aaceb3b3d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set your HF_TOKEN environment variable before running this cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integration with Chatbot\n",
        "\n",
        "The downloaded model can now be used in the Streamlit chatbot to score URLs dynamically.  \n",
        "For example, the chatbot can:\n",
        "\n",
        "- Download the latest model from Hugging Face at startup.\n",
        "- Use `assess_url_credibility()` to compute credibility scores for URLs or web results.\n",
        "- Provide users with both an answer and the credibility assessment.\n",
        "\n",
        "This ensures that the chatbot always uses a reproducible, deployment-ready model.\n"
      ],
      "metadata": {
        "id": "W08fV-8almNG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fU3VJ-DglmoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}