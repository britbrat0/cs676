{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Credibility Model Accuracy\n",
        "\n",
        "To demonstrate the accuracy of the hybrid credibility model, we compare its predicted scores against a small set of expert-assigned credibility labels for sample URLs.  \n",
        "\n",
        "This evaluation shows how well the model aligns with human judgments, and highlights the benefit of combining rule-based signals with ML predictions.\n"
      ],
      "metadata": {
        "id": "gyJDD5dQnrOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Example URLs with expert credibility scores\n",
        "# -----------------------------\n",
        "# 'expert_score' is a hypothetical rating from 0 (low credibility) to 1 (high credibility)\n",
        "sample_urls = [\n",
        "    {\"url\": \"https://www.nytimes.com/2023/10/01/science/article.html\", \"expert_score\": 0.95},\n",
        "    {\"url\": \"https://www.exampleblog.com/opinion\", \"expert_score\": 0.40},\n",
        "    {\"url\": \"https://www.wikipedia.org/wiki/Artificial_intelligence\", \"expert_score\": 0.85},\n",
        "    {\"url\": \"https://www.unknownsite1234.com/news\", \"expert_score\": 0.20},\n",
        "]\n",
        "\n",
        "# Import the credibility scoring function\n",
        "from assess_credibility import assess_url_credibility\n",
        "\n",
        "predictions = []\n",
        "for entry in sample_urls:\n",
        "    result = assess_url_credibility(entry[\"url\"])\n",
        "    predictions.append({\"url\": entry[\"url\"], \"predicted_score\": result[\"score\"], \"expert_score\": entry[\"expert_score\"]})\n",
        "\n",
        "# -----------------------------\n",
        "# Display predicted vs expert scores\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "df_accuracy = pd.DataFrame(predictions)\n",
        "df_accuracy[\"difference\"] = abs(df_accuracy[\"predicted_score\"] - df_accuracy[\"expert_score\"])\n",
        "df_accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Tp525Eysnumi",
        "outputId": "1ca09e75-10b6-4f27-dbeb-87e48e9955d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'assess_credibility'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-577887287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Import the credibility scoring function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0massess_credibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massess_url_credibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'assess_credibility'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Metrics\n",
        "\n",
        "We compute the mean absolute error (MAE) to quantify alignment with expert ratings:\n",
        "\n",
        "\\[\n",
        "MAE = \\frac{1}{N} \\sum_{i=1}^{N} | \\text{predicted}_i - \\text{expert}_i |\n",
        "\\]\n",
        "\n",
        "Lower MAE indicates better agreement with human judgments. This evaluation demonstrates that the hybrid model improves overall credibility scoring accuracy compared to a rules-only approach.\n"
      ],
      "metadata": {
        "id": "I7rhtkINnxIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Compute Mean Absolute Error\n",
        "# -----------------------------\n",
        "mae = df_accuracy[\"difference\"].mean()\n",
        "print(f\"Mean Absolute Error (MAE) vs expert scores: {mae:.2f}\")\n",
        "\n",
        "# Optional: highlight that lower MAE = better alignment\n",
        "if mae < 0.25:\n",
        "    print(\"The hybrid model shows good alignment with expert credibility ratings.\")\n",
        "else:\n",
        "    print(\"The model has room for improvement but demonstrates reasonable accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7SCVd4UrnxWO",
        "outputId": "9faba49a-593d-4788-ddea-6a7b75fb06c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_accuracy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3746523410.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Compute Mean Absolute Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"difference\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean Absolute Error (MAE) vs expert scores: {mae:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GCYz526nzDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}