{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Credibility Model Accuracy\n",
        "\n",
        "To demonstrate the accuracy of the hybrid credibility model, we compare its predicted scores against a small set of expert-assigned credibility labels for sample URLs.  \n",
        "\n",
        "This evaluation shows how well the model aligns with human judgments, and highlights the benefit of combining rule-based signals with ML predictions.\n"
      ],
      "metadata": {
        "id": "gyJDD5dQnrOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Example URLs with expert credibility scores\n",
        "# -----------------------------\n",
        "# 'expert_score' is a hypothetical rating from 0 (low credibility) to 1 (high credibility)\n",
        "sample_urls = [\n",
        "    {\"url\": \"https://www.nytimes.com/2023/10/01/science/article.html\", \"expert_score\": 0.95},\n",
        "    {\"url\": \"https://www.exampleblog.com/opinion\", \"expert_score\": 0.40},\n",
        "    {\"url\": \"https://www.wikipedia.org/wiki/Artificial_intelligence\", \"expert_score\": 0.85},\n",
        "    {\"url\": \"https://www.unknownsite1234.com/news\", \"expert_score\": 0.20},\n",
        "]\n",
        "\n",
        "# Import the credibility scoring function\n",
        "from assess_credibility import assess_url_credibility\n",
        "\n",
        "predictions = []\n",
        "for entry in sample_urls:\n",
        "    result = assess_url_credibility(entry[\"url\"])\n",
        "    predictions.append({\"url\": entry[\"url\"], \"predicted_score\": result[\"score\"], \"expert_score\": entry[\"expert_score\"]})\n",
        "\n",
        "# -----------------------------\n",
        "# Display predicted vs expert scores\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "df_accuracy = pd.DataFrame(predictions)\n",
        "df_accuracy[\"difference\"] = abs(df_accuracy[\"predicted_score\"] - df_accuracy[\"expert_score\"])\n",
        "df_accuracy\n"
      ],
      "metadata": {
        "id": "sfjpGoXIqEy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Metrics\n",
        "\n",
        "We compute the mean absolute error (MAE) to quantify alignment with expert ratings:\n",
        "\n",
        "\\[\n",
        "MAE = \\frac{1}{N} \\sum_{i=1}^{N} | \\text{predicted}_i - \\text{expert}_i |\n",
        "\\]\n",
        "\n",
        "Lower MAE indicates better agreement with human judgments. This evaluation demonstrates that the hybrid model improves overall credibility scoring accuracy compared to a rules-only approach.\n"
      ],
      "metadata": {
        "id": "I7rhtkINnxIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Compute Mean Absolute Error\n",
        "# -----------------------------\n",
        "mae = df_accuracy[\"difference\"].mean()\n",
        "print(f\"Mean Absolute Error (MAE) vs expert scores: {mae:.2f}\")\n",
        "\n",
        "# Optional: highlight that lower MAE = better alignment\n",
        "if mae < 0.25:\n",
        "    print(\"The hybrid model shows good alignment with expert credibility ratings.\")\n",
        "else:\n",
        "    print(\"The model has room for improvement but demonstrates reasonable accuracy.\")\n"
      ],
      "metadata": {
        "id": "-GCYz526nzDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}